% Encoding: ISO-8859-1


@article{tas2006,
	author = {A. F. Karr and C. N. Kohnen and A. Oganian  and  J. P. Reiter  and A. P. Sanil},
	title = {A Framework for Evaluating the Utility of Data Altered to Protect Confidentiality},
	journal = {The American Statistician},
	year = {2006},
	volume = {60},
	number = {3},
	pages = {1-9},
	doi = {10.1198/000313006X124640},
}

@article{AbowdSchmutte_BPEA2015,
	jstor_articletype = {research-article},
	title = {Economic analysis and statistical disclosure limitation},
	Author = {John M. Abowd and Ian Schmutte},
	journal = {Brookings Papers on Economic Activity},
	volume = {Fall 2015},
	url = {http://www.brookings.edu/about/projects/bpea/papers/2015/economic-analysis-statistical-disclosure-limitation},
	ISSN = {00072303},
	abstract = {This paper explores the consequences for economic research of methods used by statistical agencies to protect confidentiality of their respondents. We first review the concepts of statistical disclosure limitation for an audience of economists who may be unfamiliar with these methods. Our main objective is to shed light on the effects of statistical disclosure limitation for empirical economic research. In general, the standard approach of ignoring statistical disclosure limitation leads to incorrect inference. We formalize statistical disclosure methods in a model of the data publication process. In the model, the statistical agency collects data from a population, but published a version of the data that have been intentionally distorted. The model allows us to characterize what it means for statistical disclosure limitation to be ignorable, and to characterize what happens when it is not. We then consider the effects of statistical disclosure limitation for regression analysis, instrumental variable analysis, and regression discontinuity design. Because statistical agencies do not always report the methods they use to protect confidentiality, we use our model to characterize settings in which statistical disclosure limitation methods are discoverable; that is, they can be learned from the released data. We conclude with advice for researchers, journal editors, and statistical agencies.},
	language = {English},
	year = {2015},
	publisher = {Brookings Institution Press},
	copyright = {Copyright © 2015 Brookings Institution Press},
}

@Book{Dwork:Algorithmic:Book:2014,
	Title                    = {The Algorithmic Foundations of Differential Privacy},
	Author                   = {Cynthia Dwork and Aaron Roth},
	Publisher                = {now publishers, Inc.},
	Year                     = {2014},
	Note                     = {Also published as "Foundations and Trends in Theoretical Computer Science" Vol. 9, Nos. 3--4 (2014) 211-407.},
	
	Pages                    = {211--407},
	Url                      = {doi:10.1561/0400000042}
}
@Manual{synthpop,
	title = {synthpop: Generating Synthetic Versions of Sensitive Microdata for
	Statistical Disclosure Control},
	author = {Beata Nowok and Gillian M Raab and Joshua Snoke and Chris Dibben},
	year = {2016},
	note = {R package version 1.2-1},
	url = {https://CRAN.R-project.org/package=synthpop},
}

@InProceedings{kennickell1998multiple,
  author    = {Kennickell, Arthur B},
  title     = {Multiple imputation in the Survey of Consumer Finances},
  booktitle = {Proceedings of the Section on Survey Research Methods},
  year      = {1998},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@Article{kennickell1997multiple,
  author    = {Kennickell, Arthur B},
  title     = {Multiple imputation and disclosure protection: The case of the 1995 Survey of Consumer Finances},
  journal   = {Record Linkage Techniques},
  year      = {1997},
  volume    = {1997},
  pages     = {248--267},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@InProceedings{kennickell1991imputation,
  author    = {Kennickell, Arthur B},
  title     = {Imputation of the 1989 Survey of Consumer Finances: Stochastic relaxation and multiple imputation},
  booktitle = {Proceedings of the Survey Research Methods Section of the American Statistical Association},
  year      = {1991},
  volume    = {1},
  number    = {10},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@InProceedings{hawala2008producing,
  author    = {Hawala, Sam},
  title     = {Producing partially synthetic data to avoid disclosure},
  booktitle = {Proceedings of the Joint Statistical Meetings. Alexandria, VA: American Statistical Association},
  year      = {2008},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@Article{Ashwin2008,
  author    = {Ashwin Machanavajjhala and Daniel Kifer and John M. Abowd and Johannes Gehrke and Lars Vilhuber},
  title     = {Privacy: {T}heory meets practice on the map},
  journal   = {International Conference on Data Engineering (ICDE)},
  year      = {2008},
  pages     = {277--286},
  acmid     = {1547184},
  address   = {Washington, DC, USA},
  doi       = {10.1109/ICDE.2008.4497436},
  numpages  = {10},
  owner     = {vilhuber},
  publisher = {IEEE Computer Society},
  timestamp = {2017.07.04},
  url       = {http://dx.doi.org/10.1109/ICDE.2008.4497436},
}

@TechReport{Abowd2009NSFCensusIRS,
  author      = {Abowd, John M. and Andersson, Fredrik and Graham, Matthew and Vilhuber, Lars and Wu, Jeremy},
  title       = {Formal Privacy Guarantees and Analytical Validity of OnTheMap Public-use Data},
  institution = {NSF-Census-IRS Workshop on Synthetic Data and Confidentiality Protection 2009},
  year        = {2009},
  type        = {Presentation},
  owner       = {vilhuber},
  timestamp   = {2017.07.04},
  url         = {http://www.vrdc.cornell.edu/news/nsf-census-irs-workshop2009/program/},
}

@Misc{OnTheMap402,
  author       = {{U.S. Census Bureau}},
  title        = {OnTheMap version 3.1},
  howpublished = {online},
  year         = {2010},
  note         = {as of July 1, 2017},
  owner        = {vilhuber},
  timestamp    = {2017.07.04},
  url          = {https://lehd.ces.census.gov/announcements.html#091108},
}

@Article{ReiterJPC2010,
  author    = {Reiter, Jerome P.},
  title     = {Multiple Imputation for Disclosure Limitation: Future Research Challenges},
  journal   = {Journal of Privacy and Confidentiality},
  year      = {2010},
  volume    = {1},
  number    = {2},
  abstract  = {Statistical agencies that disseminate data to the public are ethically and often legally
required to protect the confidentiality of respondents' identities and sensitive attributes.
To satisfy these requirements, Rubin (1993), Little (1993), and Fienberg (1994) proposed
that agencies utilize multiple imputation. For example, agencies can release the units
originally surveyed with some values, such as sensitive values at high risk of disclosure
or values of key identifiers, replaced with multiple imputations. Multiple imputation
for protecting confidentiality is often called the synthetic data approach.},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
  url       = {http://repository.cmu.edu/jpc/vol1/iss2/7},
}

@TechReport{Stinson2009NSFCensusIRS,
  author      = {Stinson, Martha and Benedetto, Gary and Bjelland, Melissa},
  title       = {Summary of Methods and Preliminary Assessment of the SIPP Synthetic Beta},
  institution = {NSF-Census-IRS Workshop on Synthetic Data and Confidentiality Protection 2009},
  year        = {2009},
  type        = {Presentation},
  owner       = {vilhuber},
  timestamp   = {2017.07.04},
  url         = {http://www.vrdc.cornell.edu/news/nsf-census-irs-workshop2009/program/},
}

@TechReport{Snoke2017,
  author      = {Joshua Snoke and Gillian Raab and Beata Nowok and Chris Dibben and Aleksandra Slavkovic},
  title       = {General and specific utility measures for synthetic data},
  institution = {arXiv},
  year        = {2017},
  number      = {1604.06651v2},
  abstract    = {Data holders can produce synthetic versions of datasets when concerns about potential disclosure restrict the availability of the original records. This paper is concerned with methods to judge whether such synthetic data have a distribution that is comparable to that of the original data, what we will term general utility. We consider how general utility compares with specific utility, the similarity of results of analyses from the synthetic data and the original data. We adapt a previous general measure of data utility, the propensity score mean-squared-error (pMSE), to the specific case of synthetic data and derive its distribution for the case when the correct synthesis model is used to create the synthetic data. Our asymptotic results are confirmed by a simulation study. We also consider two specific utility measures, confidence interval overlap and standardized difference in summary statistics, which we compare with the general utility results. We present two examples examining this comparison of general and specific utility to real data syntheses and make recommendations for their use for evaluating synthetic data.},
  date        = {2016-04-22},
  eprint      = {1604.06651v2},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1604.06651v2:PDF},
  keywords    = {stat.AP},
  owner       = {vilhuber},
  timestamp   = {2017.07.04},
}

@Article{Meng1994,
  author        = {Xiao-Li Meng},
  title         = {Multiple-Imputation Inferences with Uncongenial Sources of Input},
  journal       = {Statistical Science},
  year          = {1994},
  volume        = {9},
  number        = {4},
  pages         = {538-558},
  issn          = {08834237},
  __markedentry = {[vilhuber:]},
  abstract      = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
  owner         = {vilhuber},
  publisher     = {Institute of Mathematical Statistics},
  timestamp     = {2017.07.04},
  url           = {http://www.jstor.org/stable/2246252},
}

@Article{DrechslerReiter2009,
  author    = {Drechsler, J\"org and Reiter, Jerome P.},
  title     = {Disclosure risk and data utility for partially synthetic data: An empirical study using the {German} {IAB} establishment survey.},
  journal   = {Journal of Official Statistics},
  year      = {2009},
  volume    = {25},
  pages     = {589--603},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
}

@Article{synthpop2016,
  author    = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
  title     = {synthpop : Bespoke creation of synthetic data in R},
  journal   = {Journal of Statistical Software},
  year      = {2016},
  volume    = {74},
  pages     = {1--26},
  owner     = {vilhuber},
  timestamp = {2017.07.04},
  url       = {https:
//www.jstatsoft.org/article/view/v074i11},
}

@Article{Little1993,
  author    = {Little, Roderick J. A.},
  title     = {Statistical analysis of masked data},
  journal   = jos,
  year      = {1993},
  volume    = {2},
  number    = {2},
  pages     = {407-426},
  owner     = {vilhuber},
  timestamp = {2017.07.05},
}

@Article{Rubin1993,
  author    = {Rubin, Donald B.},
  title     = {Satisfying {C}onfidentiality {C}onstraints {T}hrough {U}se of {S}ynthetic {M}ultiply-imputed {M}icrodata ({D}iscussion: {S}tatistical {D}isclosure {L}imitation)},
  journal   = jos,
  year      = {1993},
  volume    = {9},
  pages     = {461-468},
  owner     = {vilhuber},
  timestamp = {2017.07.05},
}

@TechReport{Barrientos2017,
  author        = {Andr\'es F. Barrientos and Alexander Bolton and Tom Balmat and Jerome P. Reiter and John M. de Figueiredo and Ashwin Machanavajjhala and Yan Chen and Charley Kneifel and Mark DeLong},
  title         = {A Framework for Sharing Confidential Research Data, Applied to Investigating Differential Pay by Race in the U. S. Government},
  institution   = {arXiv},
  year          = {2017},
  number        = {1705.07872v1},
  __markedentry = {[vilhuber:6]},
  abstract      = {Data stewards seeking to provide access to large-scale social science data face a difficult challenge. They have to share data in ways that protect privacy and confidentiality, are informative for many analyses and purposes, and are relatively straightforward to use by data analysts. We present a framework for addressing this challenge. The framework uses an integrated system that includes fully synthetic data intended for wide access, coupled with means for approved users to access the confidential data via secure remote access solutions, glued together by verification servers that allow users to assess the quality of their analyses with the synthetic data. We apply this framework to data on the careers of employees of the U. S. federal government, studying differentials in pay by race. The integrated system performs as intended, allowing users to explore the synthetic data for potential pay differentials and learn through verifications which findings in the synthetic data hold up in the confidential data and which do not. We find differentials across races; for example, the gap between black and white female federal employees' pay increased over the time period. We present models for generating synthetic careers and differentially private algorithms for verification of regression results.},
  eprint        = {1705.07872v1},
  eprintclass   = {stat.AP},
  eprinttype    = {arXiv},
  file          = {online:http\://arxiv.org/pdf/1705.07872v1:PDF},
  keywords      = {stat.AP},
  owner         = {vilhuber},
  timestamp     = {2017.07.05},
}

@Article{Raghunathan2001SRMI,
  author        = {Raghunathan, Trivellore E. and Lepkowski, James M. and Hoewyk, John Van and Solenberger, Peter},
  title         = {A multivariate technique for multiply imputing missing values using a sequence of regression models},
  journal       = {Survey Methodology},
  year          = {2001},
  volume        = {27},
  number        = {1},
  pages         = {85--95},
  __markedentry = {[vilhuber:6]},
  abstract      = {This article describes and evaluates a procedure for imputing missing values for a relatively complex data structure when the data are missing at random. The imputations are obtained by fitting a sequence of regression models and drawing values from the corresponding predictive distributions. The types of regression models used are linear, logistic, Poisson, generalized logit or a mixture of these depending on the type of variable being imputed. Two additional common features in the imputation process are incorporated: restriction to a relevant subpopulation for some variables and logical bounds or constraints for the imputed values. The restrictions involve subsetting the sample individuals that satisfy certain criteria while fitting the regression models. The bounds involve drawing values from a truncated predictive distribution. The development of this method was partly motivated by the analysis of two data sets which are used as illustrations. The sequential regression procedure is applied to perform multiple imputation analysis for the two applied problems. The sampling properties of inferences from multiply imputed data sets created using the sequential regression method are evaluated through simulated data sets. },
  keywords      = {Item nonresponse Missing at random Multiple imputation Nonignorable missing mechanism Regression Sampling properties and simulations.},
  owner         = {vilhuber},
  timestamp     = {2017.07.05},
}

@Article{woodcock2009distribution,
  author    = {Woodcock, Simon D and Benedetto, Gary},
  title     = {Distribution-preserving statistical disclosure limitation},
  journal   = {Computational Statistics \& Data Analysis},
  year      = {2009},
  volume    = {53},
  number    = {12},
  pages     = {4228--4242},
  owner     = {vilhuber},
  publisher = {Elsevier},
  timestamp = {2017.07.05},
}

@article{doi:10.1080/02664763.2011.584523,
author = { Jörg   Drechsler },
title = {New data dissemination approaches in old Europe – synthetic datasets for a German establishment survey},
journal = {Journal of Applied Statistics},
volume = {39},
number = {2},
pages = {243-265},
year = {2012},
doi = {10.1080/02664763.2011.584523},

URL = { 
        http://dx.doi.org/10.1080/02664763.2011.584523
    
},
eprint = { 
        http://dx.doi.org/10.1080/02664763.2011.584523
    
}

}

@techreport{AbowdVilhuber::SOLE::2016,
author = {Vilhuber, Lars and Abowd, John M.},
title = {Usage and outcomes of the Synthetic Data Server},
institution = {Cornell University, Labor Dynamics Institute},
type = {Presentation at Society of Labor Economics meetings},
year = 2016,
url = {http://hdl.handle.net/1813/43883}
}

@Comment{jabref-meta: databaseType:bibtex;}
